---
apiVersion: v1
kind: ConfigMap
metadata:
  name: airflow-config
  namespace: airflow
  labels:
    app: airflow
data:
  airflow.cfg: |
    [core]
    dags_folder = /airflow/dags
    base_log_folder = /airflow/logs
    executor = KubernetesExecutor
    plugins_folder = /airflow/plugins
    sql_alchemy_conn = postgresql+psycopg2://airflow:howeyporage1927fitz@postgres.airflow.svc.cluster.local/airflow
    load_examples = false
    [scheduler]
    child_process_log_directory = /airflow/logs/scheduler
    catchup_by_default = false
    [webserver]
    # rbac = true
    authenticate = false
    [kubernetes]
    namespace = airflow
    airflow_configmap = airflow-config
    worker_service_account_name = airflow
    worker_container_repository = wildflowerschools/wf-deep-docker
    worker_container_tag = airflow-v13
    worker_container_image_pull_policy = IfNotPresent
    # worker_dags_folder = /airflow/dags
    # worker_pods_creation_batch_size = 16
    delete_worker_pods = true
    dags_volume_host = /data/airflow/dags
    logs_volume_host = /data/airflow/logs
    in_cluster = true
    [kubernetes_node_selectors]
    # the key-value pairs to be given to worker pods.
    # the worker pods will be scheduled to the nodes of the specified key-value pairs.
    # should be supplied in the format: key = value
    [kubernetes_secrets]
    AIRFLOW_HOME = airflow-env=AIRFLOW_HOME
    HONEYCOMB_URI = airflow-env=HONEYCOMB_URI
    HONEYCOMB_TOKEN_URI = airflow-env=HONEYCOMB_TOKEN_URI
    HONEYCOMB_AUDIENCE = airflow-env=HONEYCOMB_AUDIENCE
    HONEYCOMB_CLIENT_ID = airflow-env=HONEYCOMB_CLIENT_ID
    HONEYCOMB_CLIENT_SECRET = airflow-env=HONEYCOMB_CLIENT_SECRET
    [cli]
    api_client = airflow.api.client.json_client
    endpoint_url = http://127.0.0.1:8080
    [api]
    auth_backend = airflow.api.auth.backend.default
    [admin]
    # ui to hide sensitive variable fields when set to true
    hide_sensitive_variable_fields = false
